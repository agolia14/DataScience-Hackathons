{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_parser = lambda x: pd.to_datetime(x, format=\"%d/%m/%y\")\n",
    "\n",
    "raw = pd.read_csv('train.csv', parse_dates=['checkin_date','checkout_date'], \n",
    "                  date_parser=date_parser,\n",
    "                 na_values = ['nan'])\n",
    "\n",
    "testData = pd.read_csv('test.csv', parse_dates=['checkin_date','checkout_date'], \n",
    "                  date_parser=date_parser,\n",
    "                 na_values = ['nan'])\n",
    "\n",
    "raw = raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = raw.drop(['booking_date'], axis = 1)\n",
    "testData = testData.drop(['booking_date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "days = raw.checkout_date - raw.checkin_date\n",
    "trainData['days_stayed'] = days.dt.days.values\n",
    "trainData['month'] = raw.checkin_date.dt.month.values\n",
    "trainData.drop(['checkin_date', 'checkout_date','channel_code','memberid'], axis = 1, inplace=True)\n",
    "\n",
    "days = testData.checkout_date - testData.checkin_date\n",
    "testData['days_stayed'] = days.dt.days.values\n",
    "testData['month'] = testData.checkin_date.dt.month.values\n",
    "testData.drop(['checkin_date', 'checkout_date','channel_code','memberid'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "\n",
    "col_enc = enc.fit(raw.resort_id)\n",
    "trainData.resort_id = enc.transform(raw.resort_id)\n",
    "testData.resort_id = enc.transform(testData.resort_id)\n",
    "\n",
    "col_enc = enc.fit(raw.member_age_buckets)\n",
    "trainData.member_age_buckets = enc.transform(raw.member_age_buckets)\n",
    "testData.member_age_buckets = enc.transform(testData.member_age_buckets)\n",
    "\n",
    "col_enc = enc.fit(raw.cluster_code)\n",
    "trainData.cluster_code = enc.transform(raw.cluster_code)\n",
    "testData.cluster_code = enc.transform(testData.cluster_code)\n",
    "\n",
    "col_enc = enc.fit(raw.reservationstatusid_code)\n",
    "trainData.reservationstatusid_code = enc.transform(raw.reservationstatusid_code)\n",
    "testData.reservationstatusid_code = enc.transform(testData.reservationstatusid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData['persons'] = trainData['numberofadults'] + .65*trainData['numberofchildren']\n",
    "testData['persons'] = testData['numberofadults'] + .65*testData['numberofchildren']\n",
    "\n",
    "stats = trainData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData[trainData.days_stayed>(stats['days_stayed']['mean'] + 3.5*stats['days_stayed']['std'])] = np.nan\n",
    "trainData[trainData.days_stayed<(stats['days_stayed']['mean'] - 3.5*stats['days_stayed']['std'])] = np.nan\n",
    "\n",
    "trainData[trainData.roomnights>(stats['roomnights']['mean'] + 3.5*stats['roomnights']['std'])] = np.nan\n",
    "trainData[trainData.roomnights<(stats['roomnights']['mean'] - 3.5*stats['roomnights']['std'])] = np.nan\n",
    "\n",
    "trainData[trainData.total_pax>(stats['total_pax']['mean'] + 3.5*stats['total_pax']['std'])] = np.nan\n",
    "trainData[trainData.total_pax<(stats['total_pax']['mean'] - 3.5*stats['total_pax']['std'])] = np.nan\n",
    "\n",
    "trainData[trainData.persons>(stats['persons']['mean'] + 3.5*stats['persons']['std'])] = np.nan\n",
    "trainData[trainData.persons<(stats['persons']['mean'] - 3.5*stats['persons']['std'])] = np.nan\n",
    "\n",
    "trainData[trainData.amount_spent_per_room_night_scaled>(stats['amount_spent_per_room_night_scaled']['mean'] + 3.5*stats['amount_spent_per_room_night_scaled']['std'])] = np.nan\n",
    "trainData[trainData.amount_spent_per_room_night_scaled<(stats['amount_spent_per_room_night_scaled']['mean'] - 3.5*stats['amount_spent_per_room_night_scaled']['std'])] = np.nan\n",
    "\n",
    "trainData.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trainData.drop(['amount_spent_per_room_night_scaled','reservation_id','season_holidayed_code','state_code_residence','reservationstatusid_code'], axis=1)\n",
    "y_train = trainData.amount_spent_per_room_night_scaled\n",
    "\n",
    "X_test = testData.drop(['reservation_id','season_holidayed_code','state_code_residence','reservationstatusid_code'], axis=1)\n",
    "test_reservation_id = testData.reservation_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFile(file_name):\n",
    "    result = pd.DataFrame({'reservation_id':test_reservation_id,'amount_spent_per_room_night_scaled':y_pred})\n",
    "\n",
    "    result = result[['reservation_id','amount_spent_per_room_night_scaled']]\n",
    "    result.to_csv(file_name,index=False)\n",
    "    print('Saved file: ' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: RandomForestRegressor.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(random_state=2,n_estimators=25, max_depth=12)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "saveFile('RandomForestRegressor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: XGBRegressor.csv\n"
     ]
    }
   ],
   "source": [
    "from xgboost import  XGBRegressor\n",
    "\n",
    "model = XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = .2,\n",
    "                max_depth = 20, alpha = 100, n_estimators = 24)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "saveFile('XGBRegressor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 50)                900       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 26,301\n",
      "Trainable params: 26,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(50, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(100, kernel_initializer='normal',activation='selu'))\n",
    "NN_model.add(Dense(100, kernel_initializer='normal',activation='selu'))\n",
    "NN_model.add(Dense(100, kernel_initializer='normal',activation='selu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 228246 samples, validate on 97821 samples\n",
      "Epoch 1/10\n",
      "228246/228246 [==============================] - 18s 81us/step - loss: 0.9996 - mean_squared_error: 0.9996 - val_loss: 1.0099 - val_mean_squared_error: 1.0099\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00992, saving model to Weights-001--1.00992.hdf5\n",
      "Epoch 2/10\n",
      "228246/228246 [==============================] - 16s 72us/step - loss: 0.8792 - mean_squared_error: 0.8792 - val_loss: 0.9778 - val_mean_squared_error: 0.9778\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00992 to 0.97782, saving model to Weights-002--0.97782.hdf5\n",
      "Epoch 3/10\n",
      "228246/228246 [==============================] - 17s 73us/step - loss: 0.8712 - mean_squared_error: 0.8712 - val_loss: 0.9818 - val_mean_squared_error: 0.9818\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.97782\n",
      "Epoch 4/10\n",
      " 67600/228246 [=======>......................] - ETA: 10s - loss: 0.8721 - mean_squared_error: 0.8721"
     ]
    }
   ],
   "source": [
    "NN_model.fit(X_train, y_train, epochs=10, batch_size=50, validation_split = 0.3, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wights_file = 'Weights-010--0.87062.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "y_pred = NN_model.predict(X_test)\n",
    "y_pred = y_pred.flatten()\n",
    "saveFile('NueralNetwork.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
